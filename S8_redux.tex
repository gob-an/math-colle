\documentclass[a4paper,12pt]{article}
\usepackage[margin=2.5cm]{geometry}
\usepackage[french]{babel}
\usepackage{amsmath,amssymb}
\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{mdframed}
\usepackage{stmaryrd}

% Define style for correction boxes
\definecolor{corrigecolor}{gray}{0.7}

\newmdenv[
linecolor=corrigecolor,
linewidth=2pt,
leftline=true,
rightline=false,
topline=false,
bottomline=false,
skipabove=6pt,
skipbelow=6pt,
innertopmargin=4pt,
innerbottommargin=4pt,
innerleftmargin=6pt,
innerrightmargin=4pt,
backgroundcolor=gray!5
]{correctionbox}




\title{Corrigé colle S8 \\
	MPI/MPI* du lycée Faidherbe \\
	\large Exercices 16, 17, 18, 19, 20, 21, 22}
\author{Brahim EL HAMDANI}

\begin{document}
	\maketitle
	
	\subsection*{Exercice 16}
    Soient $A, B \in M_n(\mathbb{C})$.
	Montrer que $(A \text{ et } B \text{ ont une valeur propre commune}) \iff (\exists P \neq 0, \text{ tel que } AP = PB)$.
	
	\begin{correctionbox}
		\textbf{Sens direct} : \\
		On suppose que $A$ et $B$ ont une valeur propre commune $\lambda$. \\
		Soit $X \in \mathbb{C}^n$ un vecteur propre de $A$ associé à $\lambda$, et $Y \in \mathbb{C}^n$ un vecteur propre de $B^\top$ associé à $\lambda$ (car  $\chi_B = \chi_{B^\top}$, donc $B$ et $B^\top$ ont mêmes valeurs propres). \\
		On pose $P = X Y^\top$. Vérifions que $P$ convient :
		\begin{itemize}
			\item $P \neq 0$ car $X \neq 0$ et $Y \neq 0$ (c'est pas dur à montrer)
			\item $AP = A(X Y^\top) = (AX) Y^\top = (\lambda X) Y^\top = \lambda (X Y^\top)$
			\item $PB = (X Y^\top) B = X (Y^\top B) = X (B^\top Y)^\top = X (\lambda Y)^\top = \lambda (X Y^\top)$
		\end{itemize}
		Ainsi, $AP = \lambda P = PB$, donc $AP = PB$ avec $P \neq 0$.	
		\\\\
		\textbf{Réciproque} : \\
		On suppose qu'il existe $P \neq 0$ tel que $AP = PB$. \\ On montre par récurrence (big flemme) que pour tout $k \in \mathbb{N}$, $A^k P = P B^k$ :\\ Il vient alors que pour tout polynôme $Q \in \mathbb{C}[X]$, on a $Q(A)P = P Q(B)$.\\En particulier, pour $Q = \chi_A$, on a par le théorème de Cayley-Hamilton :
		$$\chi_A(A) P = P \chi_A(B) = O_n$$
		Comme $P \neq 0$, on en déduit que $\chi_A(B)$ n'est pas inversible. \\
		
		Or, sur $\mathbb{C}$, $\chi_A$ est scindé :
		$$\chi_A(X) = \prod_{\lambda \in \mathrm{Sp}(A)} (X - \lambda)^{m_\lambda}$$\\
		
		On a donc :
		$$\chi_A(B) = \prod_{\lambda \in \mathrm{Sp}(A)} (B - \lambda I_n)^{m_\lambda}$$
		Comme $\chi_A(B)$ n'est pas inversible, il existe $\lambda \in \mathrm{Sp}(A)$ tel que $B - \lambda I_n$ n'est pas inversible (sinon chaque $B - \lambda I_n$ serait inversible et leur produit aussi). \\ Ainsi, $\ker(B - \lambda I_n) \neq \{0\}$, donc $\lambda$ est valeur propre de $B$. \\\\Conclusion : $A$ et $B$ ont une valeur propre commune.
	\end{correctionbox}



    \subsection*{Exercice 17}
    Soit \( u \in \mathcal{L}(\mathbb{R}^n) \) tel que : \( \exists q \in \mathbb{N}^* / u^q = \mathrm{Id} \). 
    Montrer que \( \dim (\ker (u - \mathrm{Id})) = \frac{1}{q} \sum_{k=1}^{q} \mathrm{Tr}(u^k) \).
    
    \begin{correctionbox}
        Soit \( A \) la matrice de \( u \) dans une base de \( \mathbb{R}^n \). \\ \\
        On a \( A^q = I_n \), donc le polynôme \( Q(X) = X^q - 1 \) est annulateur de \( A \). \\ \\
        Sur \( \mathbb{C} \), \( Q \) est scindé à racines simples (les racines \( q \)-ièmes de l'unité).Ainsi, \( A \) est diagonalisable sur \( \mathbb{C} \) et \( \mathrm{Sp}(A) \subset \{ \omega^k \mid k \in \llbracket 0, q-1 \rrbracket \} \) où \( \omega = e^{2\pi i/q} \). \\ \\
        Soit \( m(1) = \dim(\ker(u - \mathrm{Id})) \) la multiplicité de la valeur propre 1. On veut montrer que \( m(1) = \frac{1}{q} \sum_{k=1}^{q} \mathrm{Tr}(u^k) \). \\ \\
        Comme \( A \) est diagonalisable, elle est semblable à une matrice diagonale :
        \[
        \Delta = \mathrm{diag}(\lambda_1, \dots, \lambda_n) \quad \text{avec} \quad \lambda_i \in Sp(A)\quad \text{et} \quad \lambda_i^q = 1 \    
        \] \\
        On a alors pour tout \( k \in \mathbb{N}^* \) :
        \[
        \mathrm{Tr}(u^k) = \mathrm{Tr}(A^k) = \mathrm{Tr}(\Delta^k) = \sum_{i=1}^n \lambda_i^k
        \]
        D'où :
        \[
        \sum_{k=1}^{q} \mathrm{Tr}(u^k) = \sum_{k=1}^{q} \sum_{i=1}^n \lambda_i^k 
        = \sum_{i=1}^n \sum_{k=1}^{q} \lambda_i^k
        \]
        Pour chaque \( \lambda_i \), on distingue deux cas :
        \begin{itemize}
        \item Si \( \lambda_i = 1 \), alors \( \sum_{k=1}^{q} \lambda_i^k = \sum_{k=1}^{q} 1 = q \)
        \item Si \( \lambda_i \neq 1 \), alors c'est une racine \( q \)-ième de l'unité différente de 1, donc :
        \[
        \sum_{k=1}^{q} \lambda_i^k = \lambda_i \frac{1 - \lambda_i^q}{1 - \lambda_i} = \lambda_i \frac{1 - 1}{1 - \lambda_i} = 0
        \]
        \end{itemize}
        Il vient alors que :
        \[
        \sum_{k=1}^{q} \mathrm{Tr}(u^k) = m(1) \cdot q
        \]
        On en déduit :
        \[
        \dim(\ker(u - \mathrm{Id})) = m(1) = \frac{1}{q} \sum_{k=1}^{q} \mathrm{Tr}(u^k)
        \]
    \end{correctionbox}



    \subsection*{Exercice 18}
    Soient \( A \) et \( X \) dans \( M_n(\mathbb{R}) \). On suppose que \( X \) est de rang 1. Montrer que \( \det(A + X) \det(A - X) \leq \det(A)^2 \).
    
    \begin{correctionbox}
        Comme \( X \) est de rang 1, il existe \( P, Q \in GL_n(\mathbb{R}) \) telles que \( X = P J_1 Q \) 
        \\ \\
        On rappelle que \( J_1 = \begin{pmatrix} 1 & 0 & \cdots & 0 \\ 0 & 0 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 0 \end{pmatrix} \). 
        \\ \\ \\
        Considérons le polynôme \( P(t) = \det(A + tX) \). \\
        On a \( P(t) = \det(A + tP J_1 Q) = \det(P(P^{-1}AQ^{-1} + tJ_1)Q) = \det(P)\det(Q)\det(B + tJ_1) \) où \( B = P^{-1}AQ^{-1} \). \\ \\
        Développons \( \det(B + tJ_1) \) par rapport à la première colonne. Notons \( B = (b_{i,j}) \). \\ \\
        \[
        \begin{aligned}
        \det(B + tJ_1) &= \begin{vmatrix}
        b_{1,1} + t & b_{1,2} & \cdots & b_{1,n} \\
        b_{2,1}     & b_{2,2} & \cdots & b_{2,n} \\
        \vdots     & \vdots & \ddots & \vdots \\
        b_{n,1}     & b_{n,2} & \cdots & b_{n,n}
        \end{vmatrix} \\
        &= (b_{1,1} + t) \cdot \Delta_{1,1} + \sum_{k=2}^n (-1)^{k+1} b_{k,1} \cdot \Delta_{k,1}
        \end{aligned}
        \] \\ \\
        où \( \Delta_{k,1} \) est le déterminant de la matrice extraite obtenue en supprimant la k-ième ligne et la première colonne. \\ \\
        On en déduit que P(t) est de degré 1 donc : 
        \[
        \exists (a,b) \in \mathbb{R}^2, P(t) = at + b
        \]
        On a alors :
        \[
        P(1) = \det(A + X) = a + b, \quad P(-1) = \det(A - X) = b - a, \quad P(0) = \det(A) = b
        \]
        Calculons le produit :
        \[
        \det(A + X)\det(A - X) = (a + b)(b - a) = b^2 - a^2 \leq b^2 = \det(A)^2
        \]
    \end{correctionbox}



    \subsection*{Exercice 19}

        \begin{correctionbox}
        \textbf{Résultat utile:} \\
        Soient \(U,V \in \mathcal{M}_{n,1}(\mathbb{C})\). Alors \(UV^T = 0\) $\Leftrightarrow$ \(U= 0 \quad ou \quad V=0\)
        \end{correctionbox}
        
        Pour \( A \in \mathcal{M}_n(\mathbb{C}) \), on pose
        \[
        f_A : M \in \mathcal{M}_n(\mathbb{C}) \mapsto AMA^T
        \]

        \textbf{Note :} On admet que \(f_A\) est non nulle car sinon \(f_A(I_n) = AA^T = 0\) donc \(A=0\) ce qui est sans intérêt.
        
        \begin{enumerate}
        \item Montrer que \((X_i)_{1 \leq i \leq n}\) et \((Y_i)_{1 \leq i \leq n}\) sont des bases de \(\mathcal{M}_{n,1}(\mathbb{C})\) si et seulement si \((X_i Y_j^T)_{1 \leq i,j \leq n}\) est une base de \(\mathcal{M}_n(\mathbb{C})\).
        
        \begin{correctionbox}
        \textbf{Sens direct :} \\
        Supposons que \((X_i)\) et \((Y_j)\) sont des bases de \(\mathcal{M}_{n,1}(\mathbb{C})\). \\
        Toute matrice \(M \in \mathcal{M}_n(\mathbb{C})\) peut s'écrire comme combinaison linéaire de matrices de rang 1 (les matrices de la base canonique sont de rang 1 et elles engendrent \(\mathcal{M}_{n}(\mathbb{C})\)). \\
        Or, si \(M\) est de rang 1, elle s'écrit \(M = UV^T\) avec \(U,V \in \mathcal{M}_{n,1}(\mathbb{C})\). \\
        En décomposant \(U = \sum_{i=1}^n \alpha_i X_i\) et \(V = \sum_{j=1}^n \beta_j Y_j\), on obtient :
        \[
        M = \left(\sum_{i=1}^n \alpha_i X_i\right)\left(\sum_{j=1}^n \beta_j Y_j\right)^T = \sum_{i,j=1}^n \alpha_i \beta_j X_i Y_j^T
        \]
        On en déduit que toute matrice de \(\mathcal{M}_{n,1}(\mathbb{C})\) peut s'écrire comme une combinaison linéaire des \((X_i Y_j^T)\). Ainsi, \((X_i Y_j^T)\) est génératrice. Comme elle a \(n^2\) éléments, c'est une base. \\\\ \textbf{Réciproque :} \\
        Supposons que \((X_i Y_j^T)\) est une base. \\
        Soit \(\sum_{i=1}^n \lambda_i X_i = 0\). Alors \( \sum_{i=1}^n \lambda_i X_i Y_1^T = 0\), donc tous les \(\lambda_i\) sont nuls car une sous famille d'une famille libre est libre. \\
        Ainsi, \((X_i)\) est libre, donc c'est une base. Même raisonnement pour \((Y_j)\) (il suffit de prendre une combinaison linéaire nulle, de se souvenir que la transposée est linéaire donc on transpose l'égalité puis on multiplier à gauche par\(X_1\)).
        \end{correctionbox}
        
        \item Montrer que \(A \in GL_n(\mathbb{C})\) si et seulement si \(f_A\) est inversible.
        
        \begin{correctionbox}
        \textbf{Sens direct :} \\
        Si \(A \in GL_n(\mathbb{C})\), alors \(f_{A^{-1}}(M) = A^{-1}M(A^{-1})^T\) est l'inverse de \(f_A\). \\\\ \textbf{Réciproque :} \\
        Si \(f_A\) est inversible, alors \(f_A\) est surjective donc il existe \(M\) tel que \(AMA^T = I_n\), donc \(A\) est inversible.
        \end{correctionbox}
        
        \item Montrer que si \(A\) est diagonalisable, \(f_A\) l'est aussi.
        
        \begin{correctionbox}
        Si \(A\) est diagonalisable, il existe \(P \in GL_n(\mathbb{C})\) et \(D\) diagonale telles que \(A = P^{-1}DP\). \\
        Alors :
        \[
        f_A(M) = P^{-1}DP \cdot M \cdot P^{-1}DP
        \]
        On définit \(\phi_P(M) = PMP^{-1}\), qui est évidemment un automorphisme (flemme). \\
        On a \(f_A = \phi_{P^{-1}} \circ v \circ \phi_P\) où \(v(M) = DMD\). \\

        On montre aisément (flemme) que \(f_A\) est diagonalisable si et seulement si $v$ est diagonalisable.
        Or, la base canonique est une base de vecteurs propres de \(v\) (flemme, faites le calcul), donc \(v\) est diagonalisable, donc \(f_A\) aussi.
        \end{correctionbox}
        
        \item Soit \(Y\) un vecteur propre de \(A\). Montrer que \(F = \{XY^T, X \in \mathcal{M}_{n,1}(\mathbb{C})\}\) est stable par \(f_A\).
        
        \begin{correctionbox}
        Soit \(Y\) vecteur propre de \(A\) associé à \(\lambda\). \\
        Pour \(M = XY^T \in F\), on a :
        \[
        f_A(M) = A(XY^T)A^T = (AX)(Y^TA^T) = (AX)(AY)^T= (AX)(\lambda Y)^T = (\lambda AX)Y^T \in F
        \]
        Donc \(F\) est stable par \(f_A\).
        \end{correctionbox}
        
        \item Montrer que si \(f_A\) est diagonalisable, \(A\) l'est aussi.

        \begin{correctionbox}
            Supposons que \(f_A\) est diagonalisable.\\
            \begin{itemize}
                \item Si \(\mathrm{Sp}(A) = \{0\}\), alors \(A\) est nilpotente. Dans ce cas, \(f_A\) est aussi nilpotente. Or, un endomorphisme nilpotent non nul n'est pas diagonalisable. Absurde. \\
                Ainsi, il existe \(\lambda \neq 0\) dans \(\mathrm{Sp}(A)\).\\
                
                \item Soit \(Y\) un vecteur propre de \(A\) associé à \(\lambda \neq 0\).
                D'après la question 4, le sous-espace 
                \[
                F_Y = \{XY^T \mid X \in \mathcal{M}_{n,1}(\mathbb{C})\}
                \] 
                est stable par \(f_A\). \\
                
                Notons \(v = f_A|_{F_Y}\) l'endomorphisme induit par \(f_A\) sur \(F_Y\) Comme \(f_A\) est diagonalisable, \(v\) est diagonalisable. De plus, pour tout \(M = XY^T \in F_Y\), on a :
                \[
                v(M) = f_A(XY^T) = A(XY^T)A^T = (AX)(A^TY)^T = (AX)(\lambda Y)^T = \lambda (AX)Y^T
                \]
                \item Considérons l'application linéaire :
                \[
                \phi_Y : \mathcal{M}_{n,1}(\mathbb{C}) \to F_Y, \quad X \mapsto XY^T
                \] 
                • \(\phi_Y\) est injective : si \(\phi_Y(X) = 0\), alors \(XY^T = 0\). Comme \(Y \neq 0\), on a \(X = 0\). \\
                • \(\phi_Y\) est surjective par définition de \(F_Y\). \\
                Ainsi, \(\phi_Y\) est un isomorphisme. \\
                
                \item Définissons \(L : \mathcal{M}_{n,1}(\mathbb{C}) \to \mathcal{M}_{n,1}(\mathbb{C})\) par \(L(X) = \lambda AX\). \\[2pt]
                Alors pour tout \(X \in \mathcal{M}_{n,1}(\mathbb{C})\) :
                \[
                v(\phi_Y(X)) = v(XY^T) = \lambda (AX)Y^T = \phi_Y(L(X))
                \] 
                Donc \(v \circ \phi_Y = \phi_Y \circ L\), soit \(L = \phi_Y^{-1} \circ v \circ \phi_Y\). \\[2pt]
                Comme \(v\) est diagonalisable, \(L\) est diagonalisable.\\
                
                \item Posons \(T = \frac{1}{\lambda}L\), c'est-à-dire \(T(X) = AX\). \\
                Alors \(T\) est diagonalisable (car \(L\) l'est) et, coïncidence folle, \(T\) n'est autre que l'endomorphisme de \(\mathcal{M}_{n,1}(\mathbb{C})\) canoniquement associé à \(A\).\\
            \end{itemize}
            \text{Ainsi, \(A\) est diagonalisable.}
        \end{correctionbox}
    \end{enumerate}



    \subsection*{Exercice 20}
    Soit \( u \in \mathcal{L}(\mathbb{R}^4) \) tel que son polynôme caractéristique vérifie \( \chi_u(X^2) = \chi_u(X) \cdot \chi_u(X - 1) \). Déterminer les sous-espaces vectoriels de \( \mathbb{R}^4 \) stables par \( u \).
    
    \begin{correctionbox}
        En s'intéressant aux racines et au degré de \( \chi_u \), on montre que \( \chi_u = (X^2 + X + 1)^2 \) (cf exercice 10 du programme de khôlle n°5). \\\\
        Soit \( F \) un sous-espace stable de \( \mathbb{R}^4 \) par \( u \). Notons \( \tilde{u} \) la restriction de \( u \) à \( F \). \\[2pt]
        Alors \( \chi_{\tilde{u}} \mid \chi_u \), donc \( \chi_{\tilde{u}} = (X^2 + X + 1)^k \) pour \( k \in \{0, 1, 2\} \). On rappelle que \( X^2 + X + 1 \) est irréductible dans \( \mathbb{R}[X] \). De plus, on a \( \dim F = \deg \chi_{\tilde{u}} \). \\\\
        \textbf{Étude des cas :} \\[2pt]
        \begin{itemize}
            \item Si \( k = 0 \), alors \( \dim F = 0 \), donc \( F = \{0\} \). \\[2pt]
            \item Si \( k = 2 \), alors \( \dim F = 4 \), donc \( F = \mathbb{R}^4 \). \\[2pt]
            \item Si \( k = 1 \), alors \( \chi_{\tilde{u}} = X^2 + X + 1 \), donc \( \dim F = 2 \). \\[2pt]
            D'après Cayley-Hamilton, \( \tilde{u}^2 + \tilde{u} + \mathrm{Id}_F = 0 \), donc \( F \subset \ker(u^2 + u + \mathrm{Id}) \). \\[2pt]
            D'après Cayley-Hamilton, \( (u^2 + u + \mathrm{Id})^2 = 0 \), donc \( \mathrm{Im}(u^2 + u + \mathrm{Id}) \subset \ker(u^2 + u + \mathrm{Id}) \). Par le théorème du rang, \( \dim(\ker(u^2 + u + \mathrm{Id})) \geq 2 \). Raisonnons sur la dimension de \( \ker(u^2 + u + \mathrm{Id}) \) :\\\\
                • \textbf{Si \( \dim(\ker(u^2 + u + \mathrm{Id})) = 2 \)} : alors F = \( \ker(u^2 + u + \mathrm{Id}) \)\\\\
                • \textbf{Si \( \dim(\ker(u^2 + u + \mathrm{Id})) = 3 \)} : Alors \( \mathrm{rg}(u^2 + u + \mathrm{Id}) = 1 \). Comme \( \mathrm{Im}(u^2 + u + \mathrm{Id}) \) est stable par \( u \), on peut considérer l'endomorphisme induit \( w \). Alors \( \chi_w \mid \chi_u \) et \( \deg \chi_w = 1 \), ce qui est absurde car aucun polynôme de \( \mathbb{R}[X] \) de degré 1 ne divise \( (X^2 + X + 1)^2 \). \\\\
                • \textbf{Si \( \dim(\ker(u^2 + u + \mathrm{Id})) = 4 \)} : Si on prend $x \not = 0$ dans $F$, on considère la famille $(x, u(x))$. \\
                On a alors $u(x) \in \mathrm{Vect}(x, u(x))$, et $u^2(x) = -x-u(x) \in \mathrm{Vect}(x, u(x))$: l'image des éléments générateurs par $u$ est dans $\mathrm{Vect}(x, u(x))$, donc $\mathrm{Vect}(x, u(x))$ est stable par $u$. \\
                De même que précédemment, on montre que $\mathrm{Vect}(x, u(x))$ est de dimension paire. de plus, $x \not = 0$, d'où on en déduit $F = \mathrm{Vect}(x, u(x))$. \\
                Réciproquement, on vérifie que si $x \in \ker (u^2 + u + Id)$, alors $\mathrm{Vect}(x, u(x))$ est bien stable par $u$.
                \\\\
        \end{itemize}
        \textbf{Conclusion :} Les sous-espaces stables par \( u \) sont \( \{0\} \), \( \mathbb{R}^4 \), et les $\mathrm{Vect}(x, u(x))$ tels que \( x \in\ker(u^2 + u + \mathrm{Id}) \).\\\\
        \textit{Remarque}: si $\mathrm{dim} \ker (u^2 + u + \mathrm{Id}) = 2$, on a alors $\mathrm{Vect}(x, u(x)) = \ker (u^2 + u + \mathrm{Id})$.
    \end{correctionbox}



    \subsection*{Exercice 21}
    Donner une base de \( M_n(\mathbb{C}) \) telle qu'aucune matrice de cette base ne soit diagonalisable.
    
    \begin{correctionbox}
        En tant que bons flemmards, on aimerait faire l'exo sans trop se casser la tête. On aimerait donc prendre des matrices dont il est facile de dire si elles sont diagonalisables ou pas et s'arranger pour qu'elles forment une base. On pense alors à deux types de matrices :
        \begin{itemize}
        \item les matrices nilpotentes (la valeur sûre)
        \item les matrices triangulaires de rang 2 avec \( n-1 \) fois le coefficient 0 sur la diagonale (la dimension du noyau vaut alors \( n-2 \) alors que 0 est valeur propre de multiplicité \( n-1 \) donc la matrice est bien non diagonalisable)
        \end{itemize}
        
        \vspace{0.5em}
        
        \textbf{Construction de la base :}
        
        \vspace{0.5em}
        
        \begin{itemize}
        \item \textbf{Engender les coefficients non diagonaux :} On prend les matrices \( E_{i,j} \) pour \( i \neq j \). Il y a \( n^2 - n \) telles matrices.
        
        \item \textbf{Engendrer les coefficients diagonaux :} Il reste à trouver \( n \) matrices pour compléter la base. On propose les matrices suivantes : \\
          \begin{align*}
          F_1 &= E_{1,1} + E_{2,3} = \begin{pmatrix}
          1 & 0 & 0 & \cdots & 0 \\
          0 & 0 & 1 & \cdots & 0 \\
          0 & 0 & 0 & \cdots & 0 \\
          \vdots & \vdots & \vdots & \ddots & \vdots \\
          0 & 0 & 0 & \cdots & 0
          \end{pmatrix} \\\\
          F_2 &= E_{2,2} + E_{1,3} = \begin{pmatrix}
          0 & 0 & 1 & \cdots & 0 \\
          0 & 1 & 0 & \cdots & 0 \\
          0 & 0 & 0 & \cdots & 0 \\
          \vdots & \vdots & \vdots & \ddots & \vdots \\
          0 & 0 & 0 & \cdots & 0
          \end{pmatrix} \\\\
          F_i &= E_{i,i} + E_{1,2} \quad \text{pour } i \geq 3
          \end{align*}
        \end{itemize}
        
        \vspace{0.5em}
        
        \textbf{Justification de la non-diagonalisabilité :}
        
        \begin{itemize}

        \item Pour \( i \neq j \), les \( E_{i,j} \) sont nilpotentes car \( (E_{i,j})^2 = 0 \) donc elles sont non diagonalisables.
        
        \item Pour les \( F_i \) :
          \begin{itemize}
          \item \( F_1 \) a pour valeur propre 0 de multiplicité \( n-1 \) (car son polynôme caractéristique est \( X^{n-1}(X-1) \)) mais la dimension du noyau est \( n-2 \), donc elle n'est pas diagonalisable.
          \item \( F_2 \) : même raisonnement.
          \item \( F_i \) pour \( i \geq 3 \) : même raisonnement
          \end{itemize}
        \end{itemize}
        
        \vspace{0.5em}
        
        \textbf{Preuve que c'est une base :}\\
        Flemme. La famille \( \{E_{i,j}\}_{i \neq j} \cup \{F_i\}_{i=1}^n \) a \( n^2 \) éléments. La méthode classique permet de montrer qu'elle est libre.\\
        Ainsi, on a bien une base de \( M_n(\mathbb{C}) \) formée de matrices non diagonalisables.
    \end{correctionbox}



    \subsection*{Exercice 22}
    Soit \( M \in M_n(\mathbb{C}) \), \( I = \{P \in \mathbb{C}[X] \mid P(M) \text{ est nilpotent}\} \). Déterminer \( I \).
    
    \begin{correctionbox}
        Les gros bandeurs de groupe (je vise personne...) auront remarqué que $I$ s'appelle comme ça parce que c'est un idéal de \(C[X]\). Or, les idéaux de \(K[X]\) lorsque $K$ est un corps commutatif sont de la forme \(Q(X)K[X]\). On va montrer que c'est le cas ici. \\\\
        On pose \(Q(X) = \prod_{\lambda \in Sp(M)} (X - \lambda)\)\\\\
        Soit \( P \in I \). Alors \( P(M) \) est nilpotente, donc \( \mathrm{Sp}(P(M)) = \{0\} \). \\[2pt]
        Or, pour tout \( \lambda \in \mathrm{Sp}(M) \), si \( v \) est un vecteur propre associé, on a :
        \[
        P(M)v = P(\lambda)v
        \]
        Donc \( P(\lambda) \in \mathrm{Sp}(P(M)) = \{0\} \), soit \( P(\lambda) = 0 \). \\[2pt]
        Ainsi, \( P \) s'annule sur tout le spectre de \( M \), donc \( Q \mid P \) i.e. il existe \(R \in \mathbb{C}[X]\) tel que P = QR : \( P \in Q(X) \mathbb{C}[X] \). \\\\
        Montrons l'inclusion réciproque.\\\\
        Soit \( P \in Q(X) \mathbb{C}[X] \), écrivons \( P = QR \). Montrons d'abord que \( \mathrm{Sp}(P(M)) = P(\mathrm{Sp}(M)) \). \\\\
        On a déjà l'inclusion \( P(\mathrm{Sp}(M)) \subset \mathrm{Sp}(P(M)) \) d'après ce qui précède. \\\\
        Réciproquement, soit \( \mu \in \mathrm{Sp}(P(M)) \). Alors \( P(M) - \mu I_n \) n'est pas inversible. \\[2pt]
        sur \( \mathbb{C} \) tout polynôme est scindé donc on peut écrire :
        \[
        P(X) - \mu = \alpha \prod_{j=1}^d (X - r_j)
        \]
        Alors \( P(M) - \mu I_n = \alpha \prod_{j=1}^d (M - r_j I_n) \) n'est pas inversible, donc il existe \( j \) tel que \( M - r_j I_n \) n'est pas inversible i.e. \( r_j \in \mathrm{Sp}(M) \). Or, \( r_j\) est racine de \(P(X) - \mu\) donc \( P(r_j) - \mu = 0 \), soit \( \mu = P(r_j) \in P(\mathrm{Sp}(M)) \). \\\\
        Ainsi, \( \mathrm{Sp}(P(M)) = P(\mathrm{Sp}(M)) \). \\\\
        Maintenant, si \( \mu \in Sp(P(M)) \), alors il existe \(\lambda \in Sp(M)\) tel que \(\mu = P(\lambda) = Q(\lambda)R(\lambda) = 0\) car \(Q(\lambda) = 0\). \\
        On en déduit que \( \mathrm{Sp}(P(M)) = \{0\} \) donc\( P(M) \) est nilpotente.Ainsi, \( P \in I \). \\\\
        \textbf{Conclusion :} \( I = Q(X) \mathbb{C}[X] \) où \( Q(X) = \prod_{\lambda \in \mathrm{Sp}(M)} (X - \lambda) [\![ 0, q-1 ]\!] \).
    \end{correctionbox}



    \subsection*{Exercice 23}
    Soit \( A \in \mathfrak{M}_{n}(\mathbb{C}) \), \( P = \chi_{A} \), \( P_{i} = (X - \lambda_{i})^{\alpha_{i}} \) où \( \mathrm{Sp}(A) = \{\lambda_{i}\} \) et \( \alpha_{i} \) la multiplicité de \( \lambda_{i} \).  
    Soient les \( F_{i} = \mathrm{Ker} \, P_{i}(A) \).
    
    \begin{enumerate}
    \item Montrer que \( \mathbb{C}^{n} = \bigoplus_{i} F_{i} \).
    
    \begin{correctionbox}
        Flemme. Cayley-Hamilton into lemme de décomposition des noyaux. cf dernière démo de cours du groupe C du programme de khôlle.
    \end{correctionbox}
    
    \item Montrer que \( P_{i} \) est le polynôme caractéristique de \( A \) restreinte à \( F_{i} \).
    
    \begin{correctionbox}
        Les \( F_{i} \) sont stables par \( u \) (l'endomorphisme canoniquement associé à \( A \)).  
        On note \( B_{i} \) la matrice de \( u \) restreint à \( F_{i} \). Pour tout \( i \), \( \chi_{B_{i}} \) est de la forme \( (X - \lambda_{i})^{\beta_{i}} \) car \(    P_{i}\) est annulateur de \( B_{i} \) donc \( Sp(B_{i}) = \{\lambda_{i}\} \).\\\\ 
        \( \chi_{A} = \prod_{i} \chi_{B_{i}} \) car les \(F_{i}\) sont supplémentaires.  
        Or,   
        Comme \( \chi_{A} = \prod_{i} (X - \lambda_{i})^{\alpha_{i}} \), par unicité de la décomposition, on a \( \chi_{B_{i}} = (X - \lambda_{i})^{\alpha_{i}} \).  
        Ainsi, \( P_{i} \) est bien le polynôme caractéristique de \( A \) restreinte à \( F_{i} \).
    \end{correctionbox}
    
    \item Montrer que \( A = D + N \) avec \( D \) matrice diagonalisable et \( N \) nilpotente, telles que \( DN = ND \).
    
    \begin{correctionbox}
        Flemme. cf dernière démo du groupe C du programme de khôlle.
    \end{correctionbox}
    
    \item Soit \( \phi_{A} : M \mapsto AM - MA \). Exprimer la décomposition \( N + D \) de \( \phi_{A} \) en fonction de celle de \( A \).
    
    \begin{correctionbox}
        On a \( A = D + N \) avec \( D \) diagonalisable, \( N \) nilpotente, et \( DN = ND \). \\[2pt]
        Montrons que \( \phi_A = \phi_D + \phi_N \), où \( \phi_D \) est diagonalisable, \( \phi_N \) est nilpotente, et \( \phi_D \circ \phi_N = \phi_N \circ \phi_D \). \\\\
        Pour toute matrice \( M \in M_n(\mathbb{C}) \), on a :
        \[
        \phi_A(M) = AM - MA = (D + N)M - M(D + N) = (DM - MD) + (NM - MN) 
        \]
        Donc \( \phi_A = \phi_D + \phi_N \). \\[2pt]
        
        \textbf{1. Diagonalisabilité de \( \phi_D \) :} \\[2pt]
        Comme \( D \) est diagonalisable, il existe \( P \in GL_n(\mathbb{C}) \) telle que \( \Delta = P^{-1}DP \) soit diagonale. \\[2pt]
        Considérons l'automorphisme \( \Psi_P : M \mapsto PMP^{-1} \). Alors :
        \begin{align*}
            \Psi_P^{-1} \circ \phi_D \circ \Psi_P (M) &= \Psi_P^{-1} \circ \phi_D(PMP^{-1}) \\
            &= \Psi_P^{-1}(DPMP^{-1} - PMP^{-1}D) \\
            &= P^{-1}(DPMP^{-1} - PMP^{-1}D)P \\
            &= P^{-1}DPM - MP^{-1}DP \\
            &= \Delta M - M\Delta
        \end{align*}
        Notons \( \phi_\Delta : M \mapsto \Delta M - M\Delta \). \\[2pt]
        Soit \( (E_{ij}) \) la base canonique de \( M_n(\mathbb{C}) \). Si \( \Delta = \mathrm{diag}(\lambda_1, \dots, \lambda_n) \), alors :
        \[
        \phi_\Delta(E_{ij}) = \Delta E_{ij} - E_{ij} \Delta = (\lambda_i - \lambda_j) E_{ij}
        \]
        Ainsi, les \( E_{ij} \) sont des vecteurs propres de \( \phi_\Delta \), donc \( \phi_\Delta \) est diagonalisable. \\[2pt]
        Comme \( \phi_D \) est semblable à \( \phi_\Delta \), \( \phi_D \) est diagonalisable. \\[2pt]
        
        \textbf{2. Nilpotence de \( \phi_N \) :} \\[2pt]
        Si \( k \in \mathbb{N}^* \), Notons \(H_k\) :« \( \phi_N^k(M) = \sum_{j=0}^k \binom{k}{j} N^j M (-N)^{k-j} \) »\\\\\
        • Pour \( k = 1 \) : \( \phi_N(M) = NM - MN \), \(H_1\) vraie. \\\\
        • Soit \( k \geq 1 \). Supposons \(H_k\) vraie et montrons \(H_{k+1}\) :
        
        \begin{align*}
            \phi_N^{k+1}(M) &= \phi_N(\phi_N^k(M))\\ 
            &= N \left( \sum_{j=0}^k \binom{k}{j} N^j M (-N)^{k-j} \right) - \left( \sum_{j=0}^k \binom{k}{j} N^j M (-N)^{k-j} \right) N\\
            &= \sum_{j=0}^k \binom{k}{j} N^{j+1} M (-N)^{k-j} + \sum_{j=0}^k \binom{k}{j} N^j M (-N)^{k-j+1}
        \end{align*}
        
        En posant \( l = j+1 \) dans la première somme :
        \[
        = \sum_{l=1}^{k+1} \binom{k}{l-1} N^{l} M (-N)^{k-l+1} + \sum_{j=0}^k \binom{k}{j} N^j M (-N)^{k-j+1}
        \]
        
        \[
        = \sum_{j=0}^{k+1} \left[ \binom{k}{j-1} + \binom{k}{j} \right] N^j M (-N)^{k+1-j} = \sum_{j=0}^{k+1} \binom{k+1}{j} N^j M (-N)^{k+1-j}
        \]
        \textit{Remarque}: normalement, il aurait fallut sortir des sommes les termes pour l = k+1 et j = 0 afin de pouvoir appliquer la formule du triangle de Pascal et les rentrer ensuite mais flemme (faites-le pendant la khôlle).\\
        \(H_{k+1}\) vraie... principe de récurrence...bla bla $\rightarrow$ OK\\\\
        Comme \( N \) est nilpotente, soit \( p \in \mathbb{N} \) tel que \( N^p = 0 \). Alors pour \( k \geq 2p-1 \), dans chaque terme de la somme, soit \( j \geq p \), soit \( k-j \geq p \), donc \( N^j = 0 \) ou \( (-N)^{k-j} = 0 \). Ainsi, \( \phi_N^k = 0 \), et \( \phi_N \) est nilpotente. \\[2pt]
        
        \textbf{3. Commutativité de \( \phi_D \) et \( \phi_N \) :} \\
        Calculons \( \phi_D \circ \phi_N(M) \) et \( \phi_N \circ \phi_D(M) \) :
        \[
        \phi_D(\phi_N(M)) = D(NM - MN) - (NM - MN)D = DNM - DMN - NMD + MND
        \]
        \[
        \phi_N(\phi_D(M)) = N(DM - MD) - (DM - MD)N = NDM - NMD - DMN + MDN
        \]
        Comme \( DN = ND \), on a \( DNM = NDM \) et \( MND = MDN \). \\
        Ainsi, \( \phi_D \circ \phi_N(M) = \phi_N \circ \phi_D(M) \), donc \( \phi_D \circ \phi_N = \phi_N \circ \phi_D \). \\\\
        \textbf{Conclusion :} \( \phi_A = \phi_D + \phi_N \) est la décomposition de Dunford de \( \phi_A \).
    \end{correctionbox}
    \end{enumerate}
\end{document}
